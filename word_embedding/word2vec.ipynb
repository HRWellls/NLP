{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e910bda1",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae7e91",
   "metadata": {},
   "source": [
    "### 1. 安装gensim库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f854591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: gensim in /home/ma-user/anaconda3/envs/Mindquantum-0.9.0/lib/python3.9/site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/ma-user/anaconda3/envs/Mindquantum-0.9.0/lib/python3.9/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/ma-user/anaconda3/envs/Mindquantum-0.9.0/lib/python3.9/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ma-user/anaconda3/envs/Mindquantum-0.9.0/lib/python3.9/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/ma-user/anaconda3/envs/Mindquantum-0.9.0/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/Mindquantum-0.9.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e628e",
   "metadata": {},
   "source": [
    "### 2. 同步数据至本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54596adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v2.2.3.2c7f2141-2c7f2141\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n",
      "INFO:root:Using OBS-C-SDK-2.23.1\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "# mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/word_embedding/corpus.txt\", dst_url='corpus.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baec77",
   "metadata": {},
   "source": [
    "### 3. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81125cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8349f",
   "metadata": {},
   "source": [
    "### 4. 定义输入、输出文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983bec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"corpus.txt\"\n",
    "out_embedding_file = \"embedding.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbf357",
   "metadata": {},
   "source": [
    "### 5. 查看函数文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d7ad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mns_exponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhashfxn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
       "\n",
       "Warnings\n",
       "--------\n",
       "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
       "such as lambda functions etc.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
       "\n",
       "Once you're finished training a model (=no more updates, only querying)\n",
       "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
       "to reduce memory.\n",
       "\n",
       "The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
       ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
       "\n",
       "The trained word vectors can also be stored/loaded from a format compatible with the\n",
       "original word2vec implementation via `self.wv.save_word2vec_format`\n",
       "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "sentences : iterable of iterables, optional\n",
       "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
       "    consider an iterable that streams the sentences directly from disk/network.\n",
       "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
       "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
       "    See also the `tutorial on data streaming in Python\n",
       "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
       "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
       "    in some other way.\n",
       "corpus_file : str, optional\n",
       "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
       "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
       "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
       "vector_size : int, optional\n",
       "    Dimensionality of the word vectors.\n",
       "window : int, optional\n",
       "    Maximum distance between the current and predicted word within a sentence.\n",
       "min_count : int, optional\n",
       "    Ignores all words with total frequency lower than this.\n",
       "workers : int, optional\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "sg : {0, 1}, optional\n",
       "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
       "hs : {0, 1}, optional\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If 0, hierarchical softmax will not be used for model training.\n",
       "negative : int, optional\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If 0, negative sampling will not be used.\n",
       "ns_exponent : float, optional\n",
       "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
       "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
       "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
       "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
       "    other values may perform better for recommendation applications.\n",
       "cbow_mean : {0, 1}, optional\n",
       "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
       "alpha : float, optional\n",
       "    The initial learning rate.\n",
       "min_alpha : float, optional\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int, optional\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "max_vocab_size : int, optional\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "max_final_vocab : int, optional\n",
       "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
       "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
       "    Set to `None` if not required.\n",
       "sample : float, optional\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "hashfxn : function, optional\n",
       "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
       "epochs : int, optional\n",
       "    Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
       "trim_rule : function, optional\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
       "    model.\n",
       "\n",
       "    The input parameters are of the following types:\n",
       "        * `word` (str) - the word we are examining\n",
       "        * `count` (int) - the word's frequency count in the corpus\n",
       "        * `min_count` (int) - the minimum count threshold.\n",
       "sorted_vocab : {0, 1}, optional\n",
       "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
       "    See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
       "batch_words : int, optional\n",
       "    Target size (in words) for batches of examples passed to worker threads (and\n",
       "    thus cython routines).(Larger batches will be passed if individual\n",
       "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
       "compute_loss: bool, optional\n",
       "    If True, computes and stores loss value which can be retrieved using\n",
       "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
       "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
       "    Sequence of callbacks to be executed at specific stages during training.\n",
       "shrink_windows : bool, optional\n",
       "    New in 4.1. Experimental.\n",
       "    If True, the effective window size is uniformly sampled from  [1, `window`]\n",
       "    for each target word during training, to match the original word2vec algorithm's\n",
       "    approximate weighting of context words by distance. Otherwise, the effective\n",
       "    window size is always fixed to `window` words to either side.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.models import Word2Vec\n",
       "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
       "    >>> model = Word2Vec(sentences, min_count=1)\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
       "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
       "    directly to query those embeddings in various ways. See the module level docstring for examples.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/Mindquantum-0.9.0/lib/python3.9/site-packages/gensim/models/word2vec.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Doc2Vec, FastText"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32310e",
   "metadata": {},
   "source": [
    "### 6. 词向量训练并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dce3014a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 434258 word types from a corpus of 8163235 raw words and 6566 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 99249 unique words (22.85% of original 434258, drops 335009)', 'datetime': '2025-04-07T23:01:24.758362', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 7661631 word corpus (93.86% of original 8163235, drops 501604)', 'datetime': '2025-04-07T23:01:24.759677', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 434258 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7062927.526606419 word corpus (92.2%% of prior 7661631)', 'datetime': '2025-04-07T23:01:25.299316', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 99249 words and 100 dimensions: 129023700 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-07T23:01:26.125831', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 16 workers on 99249 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-04-07T23:01:26.127284', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 5.88% examples, 38961 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 51.78% examples, 283879 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 8273775 raw words (6701407 effective words) took 12.7s, 526637 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 6.66% examples, 39733 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 53.90% examples, 315428 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 8273775 raw words (6701138 effective words) took 12.3s, 542638 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 4.34% examples, 39178 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 44.88% examples, 285676 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 96.33% examples, 498253 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 8273775 raw words (6701375 effective words) took 12.6s, 529786 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 4.34% examples, 37144 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 49.21% examples, 280897 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 8273775 raw words (6702042 effective words) took 12.9s, 520464 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 3.44% examples, 39132 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 51.46% examples, 331839 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 93.97% examples, 479234 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 8273775 raw words (6701355 effective words) took 13.2s, 506308 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 41368875 raw words (33507317 effective words) took 65.1s, 514674 effective words/s', 'datetime': '2025-04-07T23:02:31.231955', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=99249, vector_size=100, alpha=0.025>', 'datetime': '2025-04-07T23:02:31.233302', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "INFO:gensim.models.keyedvectors:storing 99249x100 projection weights into embedding.txt\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(corpus_file=corpus_file, vector_size=100, window=5, min_count=5, workers=cpu_count(), sg=1)\n",
    "model.wv.save_word2vec_format(out_embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b7b6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt  embedding.txt  lost+found  word2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971d053",
   "metadata": {},
   "source": [
    "### 7. 加载离线词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0fdb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from embedding.txt\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (99249, 100) matrix of type float32 from embedding.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T23:02:56.843921', 'gensim': '4.3.3', 'python': '3.9.11 (main, Mar 29 2022, 19:08:29) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h998.eulerosv2r9.x86_64-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6171e8",
   "metadata": {},
   "source": [
    "获取单个词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91c5593f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3564694 ,  0.5670626 , -0.01008925, -0.25026983, -0.02213428,\n",
       "        0.09678983, -0.12399771,  0.12606174,  0.00477857, -0.60232115,\n",
       "        0.63458014, -0.18740645,  0.44798934, -0.15463486, -0.3450325 ,\n",
       "        0.15440823, -0.13916957, -0.4883312 ,  0.03703401,  0.17661153,\n",
       "        0.438018  ,  0.436204  ,  0.00157461, -0.05264271,  0.14773484,\n",
       "        0.39862245, -0.38107792,  0.44317234,  0.10933553, -0.17677061,\n",
       "       -0.3879517 , -0.19385327, -0.07838651, -0.6288766 , -0.23201281,\n",
       "       -0.08562619,  0.8055326 ,  0.08405694, -0.92906857, -0.16199635,\n",
       "        0.2994303 , -0.35176945,  0.01657251,  0.0460867 ,  0.14293995,\n",
       "        0.7542031 , -0.08457105, -0.35082704,  0.3487429 ,  0.05798028,\n",
       "       -0.06564034,  0.5188353 ,  0.17807192,  0.38171047, -0.3498073 ,\n",
       "       -0.29732478,  0.60607195,  0.6792102 , -0.14056094, -0.07719585,\n",
       "       -0.40230384, -0.23236197,  0.12964405,  0.02799647, -0.26935893,\n",
       "        0.70545506, -0.14668006,  0.32153648, -0.4480299 , -0.07935028,\n",
       "       -0.12906758,  0.38149363,  1.0445496 , -0.24697861,  0.08122128,\n",
       "       -0.2680164 , -0.21445757, -0.23531687, -0.80220187, -0.30550697,\n",
       "       -0.33651745,  0.26526815, -0.12333688,  0.08926912, -0.07859918,\n",
       "       -0.48485148,  0.38446358, -0.24333172,  0.52916706, -0.15976705,\n",
       "        0.66933095, -0.163606  ,  0.31173664, -0.16889077,  0.21377489,\n",
       "        0.04028728,  0.12957898, -0.15048732, -0.3715123 ,  0.35137695],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model['中国']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7da4ae",
   "metadata": {},
   "source": [
    "### 8. 相似度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80d6ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融\n",
      "[('证券期货', 0.7891009449958801), ('信贷', 0.7722412943840027), ('金融服务', 0.7718096971511841), ('房地产', 0.770221471786499), ('银行业', 0.7674803733825684), ('金融市场', 0.7662627696990967), ('国际贸易', 0.7603563666343689), ('公共事业', 0.7603023648262024), ('政策性', 0.7599055767059326), ('期货交易', 0.7596786618232727)]\n",
      "喜欢\n",
      "[('讨厌', 0.7441028356552124), ('吃喝', 0.7075539231300354), ('喝酒', 0.7046676278114319), ('不怎么', 0.6988097429275513), ('小孩子', 0.6955163478851318), ('舒服', 0.6935161352157593), ('沉溺于', 0.6913125514984131), ('鞋子', 0.6885607838630676), ('好看', 0.6885011792182922), ('很受', 0.6862788796424866)]\n",
      "中国\n",
      "[('大陆', 0.7221639156341553), ('内地', 0.6669217348098755), ('环球时报', 0.5981807112693787), ('境外', 0.5935041308403015), ('热血传奇', 0.5930512547492981), ('外国', 0.5868767499923706), ('蓝皮书', 0.5863729119300842), ('新闻事业', 0.5851414203643799), ('不派', 0.5842846035957336), ('工人日报', 0.5841678977012634)]\n",
      "北京\n",
      "[('上海', 0.7173325419425964), ('天津', 0.7127205729484558), ('上海市', 0.6791425943374634), ('北京市', 0.6776956915855408), ('南京', 0.6642477512359619), ('深圳', 0.6312263607978821), ('八宝山革命公墓', 0.6296330690383911), ('北京师范大学', 0.627811849117279), ('马相伯', 0.6261970400810242), ('北京大学', 0.6241875290870667)]\n"
     ]
    }
   ],
   "source": [
    "testwords = ['金融', '喜欢', \"中国\", \"北京\"]\n",
    "for word in testwords:\n",
    "    res = word2vec_model.most_similar(word)\n",
    "    print (word)\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ccbab",
   "metadata": {},
   "source": [
    "### 9. 词向量文件回传obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fdf436",
   "metadata": {},
   "outputs": [],
   "source": [
    "mox.file.copy_parallel(src_url=\"embedding.txt\", dst_url='s3://ascend-zyjs-dcyang/nlp/word_embedding/embedding.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3888c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mindquantum-0.9.0",
   "language": "python",
   "name": "mindquantum-0.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
