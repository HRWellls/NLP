Project Name学号姓名3220100975汪珉凯1. Project Introduction（1） 选题本项目选题为基于Transformer的中英文机器翻译。机器翻译是自然语言处理领域的核心任务之一，旨在通过神经网络模型实现不同语言之间的自动翻译。本实验重点探索Transformer在翻译任务中的应用。（2） 工作简介目标： 构建中英文双向翻译模型，实现从英文到中文的自动翻译。任务： 使用Transformer模型改进翻译效果，解决长距离依赖问题。   数据：    实验中采用中英文平行语料库（如cmn.txt和ch_en_all.txt），包含日常对话短语和句子。（3） 开发环境开发工具： Python 3.7.5 MindSpore 1.1（华为深度学习框架） JupyterLab（交互式开发环境）依赖库：基础库：numpy, re, os, unicodedata深度学习相关：mindspore.nn, mindspore.dataset, mindspore.common运行环境： 硬件：华为云ModelArts Ascend环境（兼容GPU/CPU）2. Technical Details（1） 理论知识阐述自注意力机制：	同时计算序列中所有词元的关系，无需顺序处理即可捕获长距离依赖	为每个词元生成Query、Key、Value向量计算注意力权重多头注意力：	并行多个注意力头（8头），分别学习不同层面的语言特征（如语法/语义）	各头输出拼接后通过线性投影整合位置编码：	使用正弦/余弦函数注入位置信息（弥补无时序处理的缺陷）层标准化与残差连接：	对层输入进行标准化（区别于批标准化）	残差连接缓解深层网络梯度消失问题（2） 具体算法编码器块：	多头注意力：		输入：词嵌入 + 位置编码		计算过程：			前馈网络：		含ReLU激活的双线性层：				残差连接+层标准化解码器块：	掩码多头注意力：		防止训练时看到未来词元（保持自回归特性）	编码器-解码器注意力：		Key/Value来自编码器输出，Query来自解码器输入（3） 技术细节核心函数：	MindSpore特有组件：		TransformerModel类（位于src.transformer_model）：			配置模型超参数（隐藏层大小、注意力头数等）			通过nn.Cell实现编码器/解码器堆叠		DynamicLossScaleManager：动态调整混合精度训练的损失缩放	核心计算：		注意力计算伪代码：			分数 = 矩阵乘法(Q, K转置) / sqrt(d_k)			权重 = softmax(分数)			输出 = 矩阵乘法(权重, V)		位置编码：通过预计算的正弦/余弦矩阵实现	自定义函数：		create_training_instance()：			将原始文本转为带特殊标记（<sos>、<eos>）的token ID			按max_seq_length进行序列截断/填充		load_weights()：			支持加载NPZ和MindSpore格式的模型参数			重命名参数键值以兼容（如tfm_decoder.decoder.*）训练优化：	学习率调度：前8000步线性预热，后按逆平方根衰减：	标签平滑（ε=0.1）：通过软化目标分布防止模型过度自信推理：	束搜索（宽度=4）：		每步保留概率最高的4个候选序列		使用长度惩罚系数3. Experiment Results3.1 上传notebook、data、source3.2 导入依赖库除了导入了Python基础库（os、numpy）、MindSpore深度学习框架相关模块（包括神经网络、数据类型、数据集处理、模型训练等组件），还导入了自定义模块（如tokenization、transformer_model等）。其中MindSpore部分用于构建和训练Transformer模型，包括数据处理、模型定义、优化器、损失函数和训练流程管理；自定义模块则提供了Transformer的配置实现、训练工具函数（如学习率调度、回调函数）和数据预处理功能（如tokenization和训练样本生成）。自定义代码导入部分如下：3.3设置运行环境3.4定义数据处理相关参数包括文件路径和数据路径等：3.5定义数据处理函数“data_prepare”：这段代码实现了一个文本数据预处理流程，主要功能是将原始的双语平行语料（源语言和目标语言用制表符分隔的文本）转换为MindRecord格式的训练数据集。它通过分词、长度控制、添加特殊标记等处理，将文本数据转换为模型可用的数值化特征，并按指定比例分割为训练集和验证集，最终生成二进制格式的MindRecord文件供模型训练使用。核心部分代码如下：具体注释已经给出：3.6 随机选20%的数据作为测试数据3.7 定义数据加载函数并测试之可以看到，数据被成功加载。3.8 定义训练相关超参数这一块主要定义了Transformer模型训练所需的完整配置参数，分为训练参数和模型结构参数两部分。训练参数部分设置了学习率调度策略（包括预热步数和衰减策略）、模型保存配置（如检查点保存步数和路径）、设备信息（如是否分布式训练）以及数据集相关参数（批次大小、序列长度等）；模型结构部分则提供了base和large两种规模的Transformer配置选项，分别定义了不同的隐藏层维度、注意力头数等关键架构参数，其中base版采用512维隐藏状态和6层结构，large版则使用1024维隐藏状态和更大的中间层维度，两者都配置了相同的dropout率和激活函数等超参数。部分代码如下：3.9 定义训练函数这块代码实现了Transformer模型的完整训练流程，首先加载预处理好的MindRecord格式数据集并初始化带损失函数的Transformer网络，支持从检查点恢复训练；然后配置动态学习率调度器（结合了warmup和衰减策略）和Adam优化器，设置训练回调函数包括损失监控和模型保存；最后根据是否启用混合精度训练选择不同的训练计算单元，封装成Model对象后启动多轮次训练，支持数据下沉模式以提升设备计算效率。部分核心代码如下：3.10 正式训练可以看到，在训练九分钟后，步数达到8850步，平均每一步约45毫秒。3.11 定义推理参数选取其中一部分代码进行注释添加（其他情况也是类似的设置不同的参数）：3.12 定义评估测试函数主要分为三个部分：	3.12.1  TransformerInferCell类这个类封装了Transformer模型的推理逻辑，主要作用是为预测任务提供标准化的调用接口。它继承自MindSpore的nn.Cell，通过重写construct方法实现了输入数据的预处理和模型调用的标准化流程。在推理时接收源序列ID(source_ids)和对应的注意力掩码(source_mask)，直接返回模型预测结果(predicted_ids)。这种封装使得模型调用更加简洁规范，也便于后续集成到Model类中使用。			3.12.2. load_weights函数该函数负责加载模型权重文件，支持两种格式：NumPy的.npz文件和MindSpore的checkpoint文件。主要功能包括：自动识别文件格式、处理权重名称映射（特别是解码器部分的权重名称转换）、构建参数字典，并将所有权重转换为MindSpore的Parameter格式。特别处理了嵌入层的权重名称转换，确保不同格式的预训练模型都能正确加载。最终返回包含所有模型参数的字典，可直接用于网络参数加载。	3.12.3. evaluate函数这是评估流程的主函数，实现了完整的Transformer模型评估流程：首先初始化模型和计算环境（设置为GRAPH_MODE和Ascend设备），然后加载预训练权重；接着创建数据加载器和分词器，逐批次处理测试数据；最后执行模型预测并解码输出结果。特别实现了预测结果的后处理逻辑，包括：token序列转换、特殊符号(<s></s>)的过滤处理，以及原始输入与预测结果的对比输出。整个过程包含了从数据加载、模型推理到结果可视化的完整评估链路。3.13 评估测试结果如下所示：可以看到，有时候可以较为准确的翻译，有时候则不行，但是大体上看来，基本可以翻译出一个符合中文语法的语句，尽管很多时候语言本身并不通顺。后续扩充训练数据、调节超参数、增加训练轮数可能可以改善这个问题。