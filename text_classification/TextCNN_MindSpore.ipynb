{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据同步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v2.0.1.rc0.ffd1c0c8-ffd1c0c8\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "# 请替换成自己的obs路径\n",
    "# mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/text_classification_mindspore/data/\", dst_url='./data/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'Ascend',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据预览\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "\n",
    "        '''\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # self.Vocab['None']\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        分割为训练集与测试集\n",
    "\n",
    "        '''\n",
    "\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "        # random.shuffle(self.test)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获得数据集中文字组成的词典长度\n",
    "        '''\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.train))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.test))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[  88,   64,  735 ...    0,    0,    0],\n",
      " [  15, 2866, 7683 ...    0,    0,    0],\n",
      " [   0, 3704, 5083 ...    0,    0,    0],\n",
      " ...\n",
      " [ 145,    2,   15 ...    0,    0,    0],\n",
      " [  60, 1938, 1579 ...    0,    0,    0],\n",
      " [ 102,   15, 9253 ...    0,    0,    0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, \n",
      " 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, \n",
      " 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1])}\n",
      "[   15  2866  7683  9697    72    75  2202   359    59   109    15    77\n",
      "  7807    85    32     0   220 13782 16254     4 16255    82   928   145\n",
      "   111     4     5  1495     0    74  1797     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        breakÍ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 5.3584203e-03  1.4745748e-02  1.3204108e-02 ... -9.3276221e-03\n",
      "         9.5713418e-03 -3.6102438e-03]\n",
      "       [ 4.7854390e-03  1.0139989e-02 -1.3360725e-03 ... -1.0438694e-02\n",
      "         1.1404155e-02 -5.0391690e-03]\n",
      "       [-1.8322342e-03 -2.3919381e-02 -3.2593929e-03 ... -1.0486276e-02\n",
      "        -2.5361993e-03  1.5597003e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-3.0487785e-03  7.8177312e-03  7.0068859e-03 ... -1.8605558e-04\n",
      "        -1.8689716e-02 -4.6787110e-05]\n",
      "       [-2.8451344e-03  1.8404450e-02  1.4878117e-04 ...  2.0326240e-02\n",
      "         9.2312656e-03  8.3761457e-03]\n",
      "       [-3.3368047e-03 -1.8437281e-03 -5.5684400e-04 ...  1.2744897e-02\n",
      "        -4.8814407e-03  2.3926087e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.8464256e-04 -4.6526203e-03 -1.2568114e-02 ... -2.2459365e-02\n",
      "         7.6844320e-03 -2.8864354e-02]\n",
      "       [ 3.1092109e-03  8.7425429e-03 -1.5052380e-02 ...  1.1770807e-03\n",
      "         9.4779127e-04  2.0190269e-02]\n",
      "       [-6.8535772e-03 -2.3684874e-03 -8.6177038e-03 ... -1.0015235e-03\n",
      "        -1.1133132e-02 -9.2169065e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 2.1771747e-03 -8.6177560e-03  8.2794391e-03 ... -8.1561590e-03\n",
      "         6.2805507e-03  9.6135316e-03]\n",
      "       [ 1.1537455e-02  3.8192640e-03  1.1934827e-02 ... -6.9424510e-04\n",
      "        -5.4019839e-03  1.6856944e-02]\n",
      "       [ 7.9370206e-03  2.2453019e-02  1.1611397e-03 ... -8.9332648e-03\n",
      "        -7.0447996e-03 -1.8307459e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.3086587e-02  4.8978762e-03 -1.0022106e-02 ... -1.3756386e-02\n",
      "         2.1130850e-03 -6.5443120e-03]\n",
      "       [-2.7878417e-03 -3.2323396e-03  1.6094629e-02 ... -1.0022399e-02\n",
      "        -1.9566685e-02 -1.4161346e-02]\n",
      "       [ 2.7791520e-03 -3.6301764e-03 -9.1631515e-03 ...  1.7551662e-02\n",
      "         9.7631961e-03 -4.6042055e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 5.2006957e-03 -1.1190042e-02  5.0295675e-03 ... -5.4513747e-03\n",
      "        -9.7895963e-03  1.5295449e-02]\n",
      "       [ 1.2543220e-02  1.2327123e-02 -1.9168649e-03 ...  5.9310482e-03\n",
      "         1.3346885e-02  7.7867648e-03]\n",
      "       [-3.4286662e-03  3.8972758e-03 -2.5017934e-03 ...  6.2640952e-03\n",
      "         1.1135399e-02 -3.3096171e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.29628833e-02  7.24490965e-03 -1.55654335e-02 ... -2.83121876e-03\n",
      "        -3.04981554e-03 -6.60638977e-03]\n",
      "       [ 1.05932904e-02 -4.84177901e-04 -8.58211983e-03 ... -1.54833971e-02\n",
      "        -7.70696579e-03  1.02411723e-04]\n",
      "       [-6.20884681e-03 -1.22954743e-02  7.37256464e-03 ...  2.80871126e-03\n",
      "        -1.67307481e-02 -6.77784113e-03]\n",
      "       [-1.31772319e-03  2.42899545e-03 -1.39796920e-03 ...  8.43363535e-03\n",
      "         7.57601997e-03 -1.29580637e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 2.54025310e-02 -4.14176239e-03  6.24223367e-06 ... -4.89715440e-03\n",
      "        -9.80807701e-04 -2.97437515e-03]\n",
      "       [-9.14099999e-03  7.07481196e-03 -2.65409937e-04 ...  1.22168579e-03\n",
      "         1.16343342e-03 -1.44599928e-02]\n",
      "       [ 6.61347201e-03  9.07764025e-03  4.36421903e-03 ...  6.03374932e-03\n",
      "         5.68794599e-03  2.04703994e-02]\n",
      "       [ 1.10776462e-02 -6.56592427e-03 -1.05976209e-03 ... -1.57379247e-02\n",
      "         2.94660013e-02  8.10646266e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 4.42513265e-03 -7.25236861e-03 -1.66686457e-02 ... -1.50160305e-02\n",
      "        -3.58896446e-03 -3.40649363e-04]\n",
      "       [ 1.60530210e-02 -1.60893705e-02  1.47784525e-03 ...  1.36497663e-02\n",
      "        -3.03080678e-03 -5.22183767e-03]\n",
      "       [ 1.10598523e-02  5.33120416e-04 -3.16993194e-03 ... -2.48210374e-02\n",
      "        -6.07503206e-03 -7.12805660e-04]\n",
      "       [ 7.62668904e-03 -2.35390826e-03  6.69769011e-03 ... -8.98301892e-04\n",
      "         1.26213608e-02  1.21710915e-02]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 7.97562115e-03  3.20431683e-03 -1.06392931e-02 ... -2.14703800e-03\n",
      "        -2.94926371e-02  8.74415040e-03]\n",
      "       [ 6.26575295e-03 -3.95900686e-04  8.47935956e-03 ... -6.17666729e-03\n",
      "        -1.66606884e-02  1.37073267e-02]\n",
      "       [-4.39111423e-03  4.36269585e-03 -9.77190956e-03 ... -3.99586558e-03\n",
      "        -1.68490014e-03 -7.68104102e-04]\n",
      "       [ 7.19322031e-03 -6.97819889e-03  8.78100470e-03 ... -1.47330947e-02\n",
      "        -2.84173363e-03 -1.54861389e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 2.03073863e-02 -5.83825260e-03 -1.11268344e-03 ... -3.74460756e-03\n",
      "         5.12023037e-03 -3.33412667e-03]\n",
      "       [-6.02069264e-03 -1.35793285e-02  3.16644041e-03 ... -1.79631282e-02\n",
      "         2.07717977e-02 -3.73094971e-03]\n",
      "       [-7.61228614e-03 -2.06761919e-02  7.72903301e-03 ...  6.64654071e-04\n",
      "         1.38138663e-02 -4.02594730e-03]\n",
      "       [-2.91036908e-04 -7.16233021e-03 -5.55103691e-03 ... -2.09304364e-03\n",
      "        -4.80642589e-03 -4.90864227e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-4.82749613e-03 -2.74590187e-04  2.65528727e-03 ... -9.67423711e-03\n",
      "        -5.18867234e-03 -3.18772644e-02]\n",
      "       [ 7.10259797e-03 -1.48843452e-02  6.80776779e-03 ... -7.43148429e-03\n",
      "         1.30057568e-02  9.22859181e-03]\n",
      "       [ 7.65627064e-03  3.53651657e-03  1.77188718e-03 ...  6.42936863e-03\n",
      "         1.36959478e-02  1.50422249e-02]\n",
      "       [ 7.96789303e-03  1.13933133e-02 -6.34351466e-03 ... -7.86836818e-03\n",
      "        -2.66705570e-03 -4.76497179e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-0.00396668  0.00677434  0.0222098  ...  0.0066034  -0.00607035\n",
      "         0.00044886]\n",
      "       [-0.0051812  -0.00406008 -0.00953867 ... -0.00296333 -0.00806126\n",
      "         0.00941529]\n",
      "       [-0.00649962 -0.00740991 -0.01011643 ... -0.00274356 -0.00320556\n",
      "         0.00972071]\n",
      "       [ 0.00489614  0.00304555 -0.00118533 ...  0.00448246  0.01358415\n",
      "         0.00327727]\n",
      "       [-0.0033096   0.01161905  0.00109963 ...  0.0027207   0.0090625\n",
      "         0.00820487]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00216584 -0.01094592  0.01197348 ... -0.00756484 -0.00216717\n",
      "        -0.00142244]\n",
      "       [-0.00534369 -0.00328623  0.00868835 ... -0.00074775  0.00529184\n",
      "        -0.0034178 ]\n",
      "       [-0.00055135  0.00043065  0.00213216 ... -0.00691572  0.0019348\n",
      "        -0.00784662]\n",
      "       [-0.00375489  0.0071719  -0.00702892 ...  0.00105656  0.00408167\n",
      "        -0.01538648]\n",
      "       [-0.00479263  0.01212377 -0.00501056 ... -0.0154107  -0.00776126\n",
      "        -0.00281818]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01062817  0.0059433   0.00521387 ...  0.00256356  0.00950341\n",
      "         0.01374025]\n",
      "       [-0.01110656 -0.00544995  0.00363611 ...  0.00246439  0.02366858\n",
      "         0.0101289 ]\n",
      "       [-0.00143204  0.0008275   0.00183172 ...  0.00589097 -0.00734964\n",
      "         0.01238359]\n",
      "       [ 0.01096534 -0.00801393 -0.00699196 ...  0.01036856 -0.00517653\n",
      "         0.00677857]\n",
      "       [-0.00818928 -0.00757984  0.00504638 ... -0.00311137 -0.00458085\n",
      "         0.00814066]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 0.00808535  0.00385398 -0.00233449 ...  0.01815077  0.00855992\n",
      "        -0.01450582]\n",
      "       [-0.00742837 -0.00590378  0.00314159 ...  0.01950101  0.01241874\n",
      "        -0.00499229]\n",
      "       [ 0.00162186 -0.00185776 -0.00046077 ... -0.01155128  0.00084931\n",
      "         0.00359652]\n",
      "       [ 0.01614176 -0.01030236 -0.00112351 ... -0.00807623  0.0122609\n",
      "        -0.00364557]\n",
      "       [-0.00992552 -0.00854727 -0.01942499 ... -0.00346524 -0.01492351\n",
      "         0.0172581 ]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00331553  0.01472189 -0.00406617 ... -0.00169143  0.00496163\n",
      "        -0.00622761]\n",
      "       [-0.01115198 -0.0031605   0.00503956 ...  0.00442161  0.00224836\n",
      "         0.00015651]\n",
      "       [-0.00282542 -0.00463645  0.00607561 ...  0.01583685 -0.00251981\n",
      "         0.00567269]\n",
      "       [ 0.00358195  0.01205465 -0.01101967 ... -0.01249744  0.01260353\n",
      "         0.00908387]\n",
      "       [ 0.00397865  0.00447714  0.01112536 ... -0.00137747 -0.00192305\n",
      "        -0.02306905]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00839121  0.00849627  0.00517421 ...  0.00308833 -0.0036986\n",
      "        -0.00554298]\n",
      "       [-0.0135159  -0.010587    0.01114365 ... -0.00647009 -0.01210392\n",
      "        -0.00355448]\n",
      "       [ 0.00324823  0.00668954  0.01610433 ... -0.01150228 -0.00569689\n",
      "        -0.00046653]\n",
      "       [ 0.01166165 -0.01661342  0.00194007 ... -0.01034434  0.00233456\n",
      "        -0.01472199]\n",
      "       [-0.00691648 -0.00139139 -0.00083476 ... -0.00081115  0.0054438\n",
      "        -0.00320667]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training if set pre_trained to be True\n",
    "if cfg.pre_trained:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 596, loss is 0.09109404683113098\n",
      "epoch time: 33456.352 ms, per step time: 56.135 ms\n",
      "epoch: 2 step: 596, loss is 0.018366975709795952\n",
      "epoch time: 4868.322 ms, per step time: 8.168 ms\n",
      "epoch: 3 step: 596, loss is 0.0005682941409759223\n",
      "epoch time: 4800.450 ms, per step time: 8.054 ms\n",
      "epoch: 4 step: 596, loss is 0.00037542596692219377\n",
      "epoch time: 4874.141 ms, per step time: 8.178 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\n",
      "accuracy:  {'acc': 0.74609375}\n"
     ]
    }
   ],
   "source": [
    "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=0.001, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n",
    "                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    param_dict = load_checkpoint(checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n",
    "else:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n",
    "\n",
    "load_param_into_net(net, param_dict)\n",
    "net.set_train(False)\n",
    "model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n",
    "\n",
    "acc = model.eval(dataset)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so boring\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is exciting\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so attractive\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spleeping 单词未出现在字典中\n",
      "Negative comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie makes me spleeping\"\n",
    "inference(review_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
